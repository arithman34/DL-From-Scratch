# DL From Scratch

Implementing Deep Learning models from first principles â€” no highâ€‘level frameworks, just raw NumPy and education-first clarity.

# ðŸ“– Overview

This project is a lightweight, transparent collection of deep learning components and models built entirely from scratch, using only NumPy. The goal is to demonstrate core algorithmsâ€”understand gradients, forward/backward propagation, optimizers, and network structures by directly manipulating the mathematics. This is ideal for educational purposes, for those who want to understand the inner workings of deep learning without the abstraction layers of high-level libraries.

# ðŸš€ Features

- **Autograd Tensors**: An automatic differentiation engine to compute gradients.
- **Core Components**: Implementations of layers, activation functions, loss functions, optimizers, and more.
- **Models**: Fully functional neural networks, including MLPs and CNNs.
- **Training**: Custom training loops with gradient descent and backpropagation.
- **No Dependencies**: Pure NumPy, no high-level libraries like TensorFlow or PyTorch.
- **Educational Focus**: Each component is designed to be clear and understandable, with detailed comments and explanations.

# ðŸ“š Documentation

- **Installation**: Clone the repository and install dependencies.
- **Usage**: Examples of how to use the components and models.
- **Contributing**: Guidelines for contributing to the project.
- **License**: MIT License.

# ðŸ§ª Usage Examples

Train a simple MLP on synthetic data:

```bash
python examples/train_binary_classifier.py
```

Train a CNN on MNIST:

```bash
python examples/train_digit_classifier.py
```

# ðŸŽ“ Educational Goals

- **Transparency**: Avoid black box behaviorâ€”learn by looking under the hood.
- **Flexibility**: Modify and extend components to suit your needs.
- **Understanding**: See inside activations, gradients, weight updates.

# ðŸ“‚ Project Structure

```bash

DL-From-Scratch/
â”œâ”€â”€ examples/                   # Example scripts for training models
â”œâ”€â”€ src/                        # Source code for components and models
    â”œâ”€â”€ data/                   # Data loading and preprocessing utilities
    â”œâ”€â”€ deep_learning/          # Core deep learning components
        â”œâ”€â”€ module/             # Layer implementations (Dense, Conv2D, etc.)
        â”œâ”€â”€ loss/               # Loss functions (CrossEntropy, MSE, etc.)
        â”œâ”€â”€ optimizers/         # Optimizers (SGD, Adam, etc.)
        â”œâ”€â”€ datasets/           # Dataset classes for loading data
        â”œâ”€â”€ utils/              # Utility functions and classes
        â”œâ”€â”€ functional/         # Functional API for operations
        â”œâ”€â”€ tensor/             # Autograd tensor implementation

â”œâ”€â”€ tests/                      # Unit tests for components
â”œâ”€â”€ README.md                   # Project overview and documentation
```

# ðŸ”­ Future Work
- **More Models**: Implement additional architectures like RNNs, Transformers, etc.
- **Improving Performance**: Optimize the code for speed and memory efficiency maintaining educational clarity.

# ðŸ§© Extend & Contribute
- Contributions, PRs, and issue reports are welcome!

# ðŸ“« Contact
Interested in collaborating or have questions? Reach out via GitHub issues or email.

- **Author**: Arif A. Othman
- **Email**: arithman34@hotmail.com
- **GitHub**: [arithman34](https://github.com/arithman34)

# License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
